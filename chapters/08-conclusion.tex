\section{Summary}

In this thesis, we have presented a model that utilizes previously proposed regularizations like adversarial learning and combine it with a multi-task classification objective to disentangle the latent space. We also propose a novel multi-adversary setup using a bag-of-words classification objective. In addition,  empirically show the successful disentanglement of the latent spaces, both by using auxiliary classification objective on the learned latent spaces, and also by visualizing empirical samples using T-SNE plots.

We show that the model maintains a balance between style transfer strength and content preservation, and outperforms state-of-the-art models with respect to the style transfer strength, while performing comparably on content preservation and word overlap.

We open-source our implementation under a permissive license \footnote{\url{https://github.com/vineetjohn/linguistic-style-transfer}} so that the general framework can be applied to other problems that are thematically similar, and can be re-purposed for extensions to the model, described below.


\section{Future Direction}

\subsection{Model Improvements}

Albeit effective at the task it sets out to do, there are several improvements to the model that could be evaluated improve it's efficacy

\begin{itemize}
	\item Unsupervisedly building a lexicon a list based on word association with each label from the training corpora, such that a hand-crafted lexicon like \cite{hu2004mining} for sentiment is not needed during evaluation.
	\item The existing model supports transfer between an arbitrary number of discrete labels. However, it can also be easily extended, by training a regressor in place of a classifier, to continuous labels. For instance, ratings on a scale from 1-5 in Amazon/Yelp reviews, controlling generation on a continuous scale for emotion valence etc.
	\item Although the adversarial learning process is quite effective in the current model, many others have found the adversarial objective hard to train because the family of $f$-divergences tend to max-out and provide no meaningful gradients. Using an optimal transport metric like the Wasserstein-1 (Earth Mover's Distance) metric, seems to address this problem and stabilize training \citep{arjovsky2017wasserstein, gulrajani2017improved}. This could improve the content space disentanglement of our model.
	\item Since a variational autoencoder is hard to train due to the requirement of manual crafting of KL-divergence annealing procedures, an alternative is to use a Wasserstein autoencoder \citep{tolstikhin2017wasserstein}, that uses empirical sampling from the encoded distribution and a Gaussian distribution, and can effectively replace the KL-divergence objective for VAEs.
\end{itemize}


\subsection{Other Domains of Application}

Although this thesis focuses attention mainly on sentiment transfer as a proxy for general style transfer, this model can also be  rely on modifications of latent attributes in text, like controlling the style of an author or an artist, modelling other attributes like generated sentence length.

Discourse style could also be modelled and controlled by changing the autoencoder framework to an encoder-decoder framework.
