\section{Comparison to State-of-the-Art Approaches}

The results in Table \ref{tab:comparison-previous} show that, our approach achieves a comparable content-preservation score to previous work, but a significantly better style-transfer strength.

In comparison to \cite{shen2017style}, our models obtain better scores in all the evaluation metrics on the Yelp dataset, and on all metrics except language fluency on the Amazon dataset.

Unfortunately, we find that the model described by \cite{fu2017style} performs extremely poorly for both sentiment transfer tasks. A qualitative inspection of the actual sentences generated by their model shows that it frequently resorts to simply reconstructing the source sentence, thereby producing high content preservation, word overlap and language fluency scores and correspondingly low style transfer strength scores. We also tweak the latent space size hyperparameters as described in their paper, but obtain no further improvement.

Our deterministic model is easier to train than a variational model because of the fewer hyperparameters and the absence of KL-weight annealing. It outperforms the previous state-of-the-art model \citep{shen2017style} in all respects. The variational model achieves comparable performance, while yielding even better style transfer strength and language fluency scores.

This indicates that the disentangled latent space approach we describe can be used for better style-transfer sentence generation, despite not being explicitly trained for the purpose, by simply using empirically encoded vectors from the training process.


\section{Disentangled Representation Learning}

As can be seen from the t-SNE plots in Figure \ref{fig:dae-tsne} and Figure \ref{fig:vae-tsne}, sentences with different styles are noticeably separated in a cleaner manner in the style space (LHS), but are indistinguishable in the content space (RHS). It is also evident that the latent space learned by the variational autoencoder is significantly smoother and continuous than the one learned by the deterministic autoencoder.

Since we essentially give the decoder a previously unseen combination of style and latent space at inference time, our hypothesis is that using a variational autoencoder would lead to more fluent generated sentences that are able to preserve content better. In the evaluations performed, the deterministic autoencoder proves equally potent at preserving content, and even exceeds the variational autoencoder's performance in terms of content preservation. However, the fluency of the variational autoencoder is better on average, as measure by a pre-trained language model, and the deterministic autoencoder tends to produce ungrammatical sentences as seen in the Results Table \ref{tab:comparison-previous} and the qualitative examples presented in Table \ref{tab:transfer-samples}.


\section{Latent Space Style Signal}

As observed in the results in Table \ref{tab:latent-space-classification}, the 128-dimensional content vector is not particularly discriminative for style. It achieves a classification accuracy that is slightly better than random/majority guess.

However, the 8-dimensional style vector $\bm s$, despite its low dimensionality, achieves significantly higher style classification accuracy. When combining content and style vectors, we achieve no further improvement. These results verify the effectiveness of our disentangling approach, because the content space doesn't contain style information, opposed to the style space.

This result is consistent with our original hypothesis of restricting the style signal of each encoded sentence into a contained sub-space of the latent representation.


\section{Transfer Strength vs. Content Preservation}

We observe over the course of experimentation that there is a visible trade-off between style transfer strength and content preservation metrics. For the VAE model, we tune the KL divergence weight hyperparameters for both the style and content spaces, and present results from these experiments in Table \ref{tab:kl-hyperparam-opt}.

\begin{table}[ht]
	\centering
	\begin{tabular}{| c | r | r | r | r | r |}
		\hline
		\multirow{2}{*}{
		} & \textbf{Style}     & \textbf{Content}   & \textbf{Transfer} & \textbf{Content}      & \textbf{Word}    \\
		  & \textbf{KL Weight} & \textbf{KL Weight} & \textbf{Strength} & \textbf{Preservation} & \textbf{Overlap} \\
		\hline
		\hline
		  & 0.01               & 0.01               & 0.8232            & 0.8696                & 0.2118           \\
		\hline
		  & 0.03               & 0.01               & 0.7646            & 0.8769                & 0.2314           \\
		\hline
		  & 0.03               & 0.03               & 0.8123            & 0.8724                & 0.2150           \\
		\hline
		  & 0.1                & 0.03               & 0.7707            & 0.8817                & 0.2334           \\
		\hline
		  & 0.3                & 0.03               & 0.7254            & 0.8783                & 0.2357           \\
		\hline
		  & 0.03               & 0.1                & 0.9052            & 0.8546                & 0.1617           \\
		\hline
		  & 0.1                & 0.1                & 0.8474            & 0.8609                & 0.1941           \\
		\hline
		  & 0.3                & 0.1                & 0.7682            & 0.8645                & 0.2166           \\
		\hline
		  & 0.03               & 0.3                & 0.9434            & 0.7538                & 0.0516           \\
		\hline
		  & 0.1                & 0.3                & 0.9523            & 0.7533                & 0.0707           \\
		\hline
		  & 0.3                & 0.3                & 0.9122            & 0.8069                & 0.1326           \\
		\hline
	\end{tabular}
	\caption{Results - KL Divergence Weight Hyperparameter Optimisation}
	\label{tab:kl-hyperparam-opt}
\end{table}

We observe that as a general rule, imposing heavier KL divergence constraints on the content embedding space results in improvements in style transfer-strength. However, this also gradually results in posterior when collapse the content KL divergence weight is too high. This results in random noise being encoded in the content space, and the decoder produces random sentences, albeit with a significantly higher transfer strength. For the content KL-weight value 0.3, we obtain content preservation scores of $\approx 0.75$, which, as shown in Section \ref{sec:evaluation-metrics} is close to our approximate lower bound for the content preservation metric. This implies that the content of generated sentences for that model is vastly different from the original sentences. Conversely, when the KL divergence constraints are relaxed and we approach a deterministic autoencoder, the content preservation improves and the style transfer strength drops.

In our final VAE model, we set both style and content KL-weights to $0.03$, but also append the latent vector $[\bm s; \bm c]$ to the hidden state of the decoder at each time step, which improves the performance of both the style transfer strength and content preservation to better scores than those reported in Table \ref{tab:kl-hyperparam-opt}.


\section{Negative Results}

\subsection{Style Dropout}

Although the idea of regularization by dropout \citep{srivastava2014dropout} has been effectively used across neural network models and time-distributed word dropout has been used similarly for text generation tasks \citep{dai2015semi, bowman2016generating}, the empirical results for style dropout prove significantly less effective than expected, in comparison to the other model variants tested in this work.

We perform a simple ablation test on the VAE model with style KL-weight as $0.1$ and content KL-weight as $0.03$ to test the effect of the addition of the style-dropout regularization, and the results are presented in Table \ref{tab:style-dropout-results}.

\begin{table}[ht]
	\centering
	\begin{tabular}{| l | r | r | r |}
		\hline
		\multirow{2}{*}{
		\textbf{Model Variant}} & \textbf{Transfer} & \textbf{Content}      & \textbf{Word}    \\
		                        & \textbf{Strength} & \textbf{Preservation} & \textbf{Overlap} \\
		\hline
		\hline
		Without Style Dropout   & 0.7707            & 0.8817                & 0.2334           \\
		\hline
		With Style Dropout      & 0.5726            & 0.8816                & 0.2404           \\
		\hline
	\end{tabular}
	\caption{Results - Style Dropout Effect}
	\label{tab:style-dropout-results}
\end{table}

As is clear from the results, style dropout only offers a marginally better word overlap score and is comparable on the content preservation metric. However, it performs significantly worse in terms of style transfer strength.

A possible reason for this is that the spaces are already well regularized by the existing objectives and the fact that the content vector is an order of magnitude larger in size compared to the style vector makes it difficult for the model to ignore the content vector and encode garbage into it. Hence, this negates the need for additional regularization to ensure that the content vector is used by the autoencoding model.

\subsection{Nearest Neighbour Algorithm}

We also test the VAE model using the nearest neighbour inference algorithm described in Section \ref{ssec:nearest-neighbour-inference}. We utilize a VAE model with style KL-weight as $0.03$ and content KL-weight as $0.03$ for these tests, in which we vary the number of neighbour embeddings considered while selecting a style embedding with which to generate the new sentence. We use a VAE model in which the latent vector is used only for the initial state of the decoder.

The results are presented in Table \ref{tab:nearest-neighbour-inference-results}, where $N$ is the number of neighbours considered. `All' neighbours just means the empirical mean style embedding is used as in the results described in the previous result tables.

\begin{table}[ht]
	\centering
	\begin{tabular}{| r | r | r | r |}
		\hline
		\multirow{2}{*}{
		\textbf{$\bm N$}} & \textbf{Transfer} & \textbf{Content}      & \textbf{Word}    \\
		            & \textbf{Strength} & \textbf{Preservation} & \textbf{Overlap} \\
		\hline
		\hline
		1           & 0.8040            & 0.8703                & 0.1823           \\
		\hline
		10          & 0.7909            & 0.8762                & 0.1859           \\
		\hline
		100         & 0.8089            & 0.8792                & 0.1977           \\
		\hline
		All         & 0.8123            & 0.8724                & 0.2150           \\
		\hline
	\end{tabular}
	\caption{Results - Nearest Neighbour Inference}
	\label{tab:nearest-neighbour-inference-results}
\end{table}

We observe that the empirical mean embedding that considers all training examples performs even marginally better than a specialized nearest-neighbour algorithm. In addition, it is easier to implement and more computationally efficient as neighbour similarity need not be computed at inference time.


In this chapter, we have presented an analysis of our experimental results. In the next chapter, we summarize our contributions and conclude this work by presenting avenues for extension and improvement on the current model.
