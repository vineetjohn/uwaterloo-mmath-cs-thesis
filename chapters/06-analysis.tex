\section{Disentangled Representation Learning}

As can be seen from the T-SNE plots in Figure \ref{fig:dae-tsne} and Figure \ref{fig:vae-tsne}, sentences with different styles are noticeably separated in a cleaner manner in the style space (LHS), but are indistinguishable in the content space (RHS). It is also evident that the latent space learned by the variational autoencoder is significantly smoother and continuous than the one learned by the deterministic autoencoder.

Since we are essentially given the decoder a previously unseen combination of style and latent space at inference time, our hypothesis is that using a variational autoencoder would lead to more fluent generated sentences that are able to preserve content better. However, in the evaluations performed, the deterministic autoencoder proves equally potent at preserving content, and even exceeds the variational autoencoder's performance in terms of content preservation, as seen in the results table \ref{tab:comparison-previous}.


\section{Latent Space Style Signal}

As observed in the results in Table \ref{tab:latent-space-classification}, the 128-dimensional content vector is not particularly discriminative for style. It achieves a classification accuracy that is slightly better than random/majority guess.

However, the 8-dimensional style vector $\bm s$, despite its low dimensionality, achieves significantly higher style classification accuracy. When combining content and style vectors, we achieve no further improvement. These results verify the effectiveness of our disentangling approach, because the content space doesn't contain style information, opposed to the style space.

This result is consistent with our original hypothesis of restricting the style signal of each encoded sentence into a contained sub-space of the latent representation.


\section{Transfer Strength vs. Content Preservation}

We observe over the course of experimentation that there is a visible trade-off between style transfer strength and content preservation metrics. For the VAE model, we tune the KL-divergence weight hyper-parameters for both the style and content spaces.

\begin{table}[ht]
	\centering
	\begin{tabular}{| c | r | r | r | r | r |}
		\hline
		\multirow{2}{*}{
		} & \textbf{Style}     & \textbf{Content}   & \textbf{Transfer} & \textbf{Content}      & \textbf{Word}    \\
		  & \textbf{KL Weight} & \textbf{KL Weight} & \textbf{Strength} & \textbf{Preservation} & \textbf{Overlap} \\
		\hline
		\hline
		  & 0.01               & 0.01               & 0.8232            & 0.8696                & 0.2118           \\
		\hline
		  & 0.03               & 0.01               & 0.7646            & 0.8769                & 0.2314           \\
		\hline
		  & 0.03               & 0.03               & 0.8123            & 0.8724                & 0.2150           \\
		\hline
		  & 0.1                & 0.03               & 0.7707            & 0.8817                & 0.2334           \\
		\hline
		  & 0.3                & 0.03               & 0.7254            & 0.8783                & 0.2357           \\
		\hline
		  & 0.03               & 0.1                & 0.9052            & 0.8546                & 0.1617           \\
		\hline
		  & 0.1                & 0.1                & 0.8474            & 0.8609                & 0.1941           \\
		\hline
		  & 0.3                & 0.1                & 0.7682            & 0.8645                & 0.2166           \\
		\hline
		  & 0.03               & 0.3                & 0.9434            & 0.7538                & 0.0516           \\
		\hline
		  & 0.1                & 0.3                & 0.9523            & 0.7533                & 0.0707           \\
		\hline
		  & 0.3                & 0.3                & 0.9122            & 0.8069                & 0.1326           \\
		\hline
	\end{tabular}
	\caption{KL-weight hyper-parameter optimisation}
	\label{tab:kl-hyperparam-opt}
\end{table}

We observe that as a general rule, improvements in style transfer-strength usually result by imposing heavier KL-divergence constraints on the content embedding space gradually results in posterior collapse, with noise being encoded in the content space and all of the style and content being encoded in the style space. Conversely, when the KL-divergence constraints are relaxed and we approach a deterministic autoencoder, the content preservation improves and the style transfer strength drops.


\section{Comparison to State-of-the-Art Approaches}

The results in Table \ref{tab:comparison-previous} show that, our approach achieves a comparable content-preservation score to previous work, but a significantly better style-transfer strength.

The model we describe is able to obtain better scores at all the evaluation metrics than the model described by \cite{shen2017style}. Our model is also significantly better at transferring style when compared to the style embedding model proposed by \cite{fu2017style}, while performing lower on the content preservation aspect. However, a qualitative inspection of the actual sentences generated by their model shows that the model degenerates to simply reconstructing the source sentence, thereby producing high content preservation scores and correspondingly low style transfer strength scores.

Our deterministic model is easier to train than a variational model because of the fewer hyper-parameters and the absence of KL-weight annealing  and also outperforms state-of-the-art models, and the variational model achieves comparable performance. This indicates that the disentangled latent space approach we describe can be used for better style-transfer sentence generation, despite not being explicitly trained for the purpose, by simply using empirically encoded vectors from the training process.


\section{Negative Results}

\subsection{Style Dropout}

Although the idea of regularization by dropout \citep{srivastava2014dropout} has been effectively used across neural network models and time-distributed word dropout has been used similarly for text generation tasks \citep{dai2015semi, bowman2016generating}, the empirical results for style dropout prove significantly less effective than expected in comparison to the other model variants tested in this work.

We perform a simple ablation test on the VAE model with style KL-weight as $0.1$ and content KL-weight as $0.03$ to test the effect of the addition of the style-dropout regularization, and the results are presented in Table \ref{tab:style-dropout-results}.

\begin{table}[ht]
	\centering
	\begin{tabular}{| l | r | r | r |}
		\hline
		\multirow{2}{*}{
		\textbf{Model Variant}} & \textbf{Transfer} & \textbf{Content}      & \textbf{Word}    \\
		                        & \textbf{Strength} & \textbf{Preservation} & \textbf{Overlap} \\
		\hline
		\hline
		Without Style Dropout   & 0.7707            & 0.8816                & 0.2333           \\
		\hline
		With Style Dropout      & 0.5726            & 0.8816                & 0.2404           \\
		\hline
	\end{tabular}
	\caption{Style Dropout Effect}
	\label{tab:style-dropout-results}
\end{table}

As is clear from the results, style dropout only offers a marginally better word overlap score and is comparable on the content preservation metric. However, it performs significantly worse in terms of style transfer strength.

A possible reason for this is that the spaces are already well regularized by the existing objectives and the fact that the content vector is an order of magnitude larger in size compared to the style vector makes it difficult for the model to ignore the content vector and encode garbage into it. Hence, this negates the need for additional regularization to ensure that the content vector is used by the autoencoding model.

\subsection{Nearest Neighbour Algorithm}

We also test the VAE model using the nearest neighbour inference algorithm described in Section \ref{ssec:nearest-neighbour-inference}. We utilize a VAE model with style KL-weight as $0.03$ and content KL-weight as $0.03$ for these tests, in which we vary the number of neighbour embeddings considered while selecting a style embedding with which to generate the new sentence. We use a VAE model in which the latent vector is used only for the initial state of the decoder.

The results are presented in Table \ref{tab:nearest-neighbour-inference-results}, where $N$ is the number of neighbours considered. `All' neighbours just means the empirical mean style embedding is used as in the results described in the above sections.

\begin{table}[ht]
	\centering
	\begin{tabular}{| l | r | r | r |}
		\hline
		\multirow{2}{*}{
		\textbf{N}} & \textbf{Transfer} & \textbf{Content}      & \textbf{Word}    \\
		            & \textbf{Strength} & \textbf{Preservation} & \textbf{Overlap} \\
		\hline
		\hline
		1           & 0.8040            & 0.8703                & 0.1823           \\
		\hline
		10          & 0.7909            & 0.8762                & 0.1859           \\
		\hline
		100         & 0.8089            & 0.8792                & 0.1977           \\
		\hline
		All         & 0.8122            & 0.8723                & 0.2149           \\
		\hline
	\end{tabular}
	\caption{Nearest Neighbour Inference Results}
	\label{tab:nearest-neighbour-inference-results}
\end{table}

We observe that the empirical mean embedding that considers all training examples performs even marginally better than a specialized nearest-neighbour algorithm. In addition, it is easier to implement and more computationally efficient as neighbour similarity need not be computed at inference time.


In the next chapter, we summarize the contributions and conclude this work by presenting avenues for extension and improvement on the current model.
