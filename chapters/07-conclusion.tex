\section{Summary}

In this thesis, we have presented a model that utilizes previously proposed representation learning objectives like multi-task classification and adversarial learning to disentangle the latent space of a language model. We also propose a novel multi-adversary setup using a bag-of-words classification objective. We empirically show the successful disentanglement of the latent spaces, both by using auxiliary classification objectives on the learned latent spaces, and also by visualizing empirical samples using t-SNE plots.

We show that the model maintains a balance between style transfer strength and content preservation, and outperforms state-of-the-art models with respect to the style transfer strength, while performing comparably on content preservation and word overlap. In terms of fluency as measured by the Kneser-Ney language model, our variational autoencoder model outperforms all other models, while simultaneously obtaining significantly better style transfer strength scores. The deterministic autoencoder model performs slightly worse in terms of style transfer strength and fluency, but still eclipses the previous state-of-the-art, in terms of style transfer strength, content preservation and word overlap. We also quantitatively demonstrate, using ablation experiments, that the addition of each of the training objectives we propose individually improves the style transfer capabilities of the model.

We open-source our implementation under a permissive license \footnote{\url{https://github.com/vineetjohn/linguistic-style-transfer}} so that the general framework can be applied to other problems that are thematically similar, and can be re-purposed for extensions to the model, described below.


\section{Future Direction}

\subsection{Model Improvements}

Albeit effective at the task it sets out to do, there are several improvements to the model that could be evaluated to improve its efficacy, including:
\begin{itemize}
	\item Unsupervisedly building a lexicon based on word association with each label from the training corpora, such that a hand-crafted lexicon like \cite{hu2004mining} for sentiment is not needed during evaluation.
	\item The existing model supports transfer between an arbitrary number of discrete labels. However, it can also be easily extended, by training a regressor in place of a classifier, to continuous labels. For instance, ratings on a scale from 1-5 in Amazon/Yelp reviews, controlling generation on a continuous scale for emotion valence etc.
	\item Although the adversarial learning process is quite effective in the current model, many others have found the adversarial objective hard to train because the family of $f$-divergences tend to max-out and provide no meaningful gradients. Using an optimal transport metric like the Wasserstein-1 (Earth Mover's Distance) metric seems to address this problem and stabilize training \citep{arjovsky2017wasserstein, gulrajani2017improved}. This could improve the content space disentanglement of our model.
	\item Since a variational autoencoder is hard to train due to the requirement of manually crafting KL divergence annealing procedures, an alternative is to use a Wasserstein autoencoder \citep{tolstikhin2017wasserstein}, that uses empirical sampling from the encoded distribution and a Gaussian distribution, and can effectively replace the KL divergence objective for VAEs.
\end{itemize}


\subsection{Other Domains of Application}

Although this thesis focuses attention mainly on sentiment transfer as a proxy for general style transfer, this model can also be used for modifying latent attributes in text, like controlling the style of an author or an artist and modelling  attributes like generated sentence length. Discourse style could also be modelled and controlled by changing the autoencoder framework to an encoder-decoder framework.
