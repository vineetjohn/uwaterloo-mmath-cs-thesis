This paper tackles the problem of disentangling the style and content latent spaces in text modelling. We propose a simple and effective approach, which incorporates auxiliary losses: multi-task loss, and dual adversarial loss for label prediction and bag-of-words prediction. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled from each other using this approach.

We also apply our disentangled latent space to attribute/style transfer in natural language generation. We achieve similar content preservation scores compared to previous state-of-the-art approaches, but significantly better style-transfer strength.

Our code is made publicly available for reproduction and extension purposes.
