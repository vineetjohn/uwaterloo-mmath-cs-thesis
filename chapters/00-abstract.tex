This thesis tackles the problem of disentangling the style and content latent spaces of data in a language modelling context. We propose a simple and effective approach, which incorporates auxiliary objectives: a multi-task classification objective, and dual adversarial objectives for label prediction and bag-of-words prediction, respectively. We show, both qualitatively and quantitatively, that the style and content are indeed disentangled from each other using this approach.

We apply this disentangled latent representation learning method to attribute/label/style transfer in natural language generation. We achieve similar content preservation scores compared to previous state-of-the-art approaches, and significantly better style-transfer strength.

Our code is made publicly available for reproduction and extension purposes.

